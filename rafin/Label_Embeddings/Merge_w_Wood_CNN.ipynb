{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import models, layers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rafin\\AppData\\Local\\Temp\\ipykernel_10360\\3036340341.py:4: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto(gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8))\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the embeddings\n",
    "loaded_embeddings = np.load(\"Wood_Embeddings.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['heartwood', 'sapwood'])\n"
     ]
    }
   ],
   "source": [
    "print(loaded_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heartwood Embeddings Shape: torch.Size([1024])\n",
      "Sapwood Embeddings Shape: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "heartwood_embeddings = loaded_embeddings['heartwood']\n",
    "sapwood_embeddings = loaded_embeddings['sapwood']\n",
    "\n",
    "# Verify their shapes or contents if needed\n",
    "print(f\"Heartwood Embeddings Shape: {heartwood_embeddings.shape}\")\n",
    "print(f\"Sapwood Embeddings Shape: {sapwood_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection to match FC weights size\n",
    "projection_layer = nn.Linear(1024, )  # Adjust output dim to match your FC\n",
    "\n",
    "sapwood_emb_projected = projection_layer(heartwood_embeddings)\n",
    "heartwood_emb_projected = projection_layer(heartwood_embeddings)\n",
    "\n",
    "# Convert PyTorch tensors to NumPy arrays\n",
    "sapwood_emb_projected = sapwood_emb_projected.detach().cpu().numpy()\n",
    "heartwood_emb_projected = heartwood_emb_projected.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heartwood Embeddings Shape: (128,)\n",
      "Sapwood Embeddings Shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "# Verify their shapes\n",
    "print(f\"Heartwood Embeddings Shape: {sapwood_emb_projected.shape}\")\n",
    "print(f\"Sapwood Embeddings Shape: {heartwood_emb_projected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 32, 32, 199, 1) (132, 32, 32, 199, 1) (264,) (132,)\n"
     ]
    }
   ],
   "source": [
    "#LOAD PCA DATA\n",
    "data_train = np.load('data_train_PCA.npy')\n",
    "data_test = np.load('data_test_PCA.npy')\n",
    "\n",
    "train_labels = np.load('Wood_train_labels.npy', allow_pickle= True)\n",
    "test_labels = np.load('Wood_test_labels.npy', allow_pickle= True)\n",
    "\n",
    "print(data_train.shape, data_test.shape, train_labels.shape, test_labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "le= LabelEncoder()\n",
    "y_train = le.fit_transform(train_labels)\n",
    "\n",
    "le2= LabelEncoder()\n",
    "y_test= le2.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def augment_data(sample, label):\n",
    "    \n",
    "    # Random flip along different axes\n",
    "    sample = tf.image.random_flip_left_right(sample)\n",
    "    sample = tf.image.random_flip_up_down(sample)\n",
    "\n",
    "    # Random rotation\n",
    "    sample = tf.image.rot90(sample, k=np.random.randint(1, 4))  # k=1 to 3 random rotations of 90°\n",
    "\n",
    "    # Random intensity scaling (brightness variation)\n",
    "    sample = sample * tf.random.uniform([], 0.9, 1.1)  # Random scale between 0.9 and 1.1\n",
    "    \n",
    "    return sample, label\n",
    "\n",
    "'''\n",
    "def augment_data(sample, label):\n",
    "    \n",
    "    # Random flip along different axes\n",
    "    sample = tf.image.random_flip_left_right(sample)\n",
    "    sample = tf.image.random_flip_up_down(sample)\n",
    "\n",
    "    # Random rotation\n",
    "    sample = tf.image.rot90(sample, k=np.random.randint(1, 4))  # k=1 to 3 random rotations of 90°\n",
    "\n",
    "    # Random intensity scaling (brightness variation)\n",
    "    sample = sample * tf.random.uniform([], 0.9, 1.1)  # Random scale between 0.9 and 1.1\n",
    "\n",
    "    # Ensure the shape is correct\n",
    "    sample = tf.transpose(sample, perm=[0, 1, 2, 3])  # Reorder dimensions\n",
    "\n",
    "    return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def fix_shape(sample, label):\n",
    "    # Reorder dimensions to match the model's expected input\n",
    "    sample = tf.transpose(sample, perm=[0, 1, 3, 2])\n",
    "    \n",
    "    return sample, label\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to TensorFlow dataset\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((data_test, y_test))\n",
    "\n",
    "'''\n",
    "# Fix train and test datasets\n",
    "train_dataset = train_dataset.map(fix_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(fix_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "'''\n",
    "\n",
    "# Apply augmentation only on training data\n",
    "train_dataset = train_dataset.map(augment_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle, batch, and prefetch\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (8, 32, 32, 199, 1)\n",
      "Batch shape: (8, 32, 32, 199, 1)\n",
      "Batch shape: (8, 32, 32, 199, 1)\n",
      "Batch shape: (8, 32, 32, 199, 1)\n",
      "Batch shape: (8, 32, 32, 199, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch, _ in train_dataset.take(5):\n",
    "    \n",
    "    print(\"Batch shape:\", batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: (8, 32, 32, 199, 1), Label batch shape: (8,)\n",
      "Input batch shape: (8, 32, 32, 199, 1), Label batch shape: (8,)\n",
      "Input batch shape: (8, 32, 32, 199, 1), Label batch shape: (8,)\n",
      "Input batch shape: (8, 32, 32, 199, 1), Label batch shape: (8,)\n",
      "Input batch shape: (8, 32, 32, 199, 1), Label batch shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(5):\n",
    "    \n",
    "    print(f\"Input batch shape: {x.shape}, Label batch shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, base_model, sapwood_emb, heartwood_emb):\n",
    "        \n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        # Pre-computed embeddings for labels\n",
    "        self.sapwood_emb = tf.constant(sapwood_emb.detach().cpu().numpy(), dtype=tf.float32)\n",
    "        self.heartwood_emb = tf.constant(heartwood_emb.detach().cpu().numpy(), dtype=tf.float32)\n",
    "\n",
    "\n",
    "        # Projection layer to align dimensions\n",
    "        self.projection_layer = tf.keras.layers.Dense(128, activation=None)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Forward pass through CNN\n",
    "        logits = self.base_model(inputs)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Project embeddings to match FC layer\n",
    "        sapwood_proj = self.projection_layer(self.sapwood_emb)\n",
    "        heartwood_proj = self.projection_layer(self.heartwood_emb)\n",
    "\n",
    "        # Extract FC layer weights\n",
    "        fc_weights = self.base_model.layers[-1].kernel  # Shape: (2, 128)\n",
    "\n",
    "        # Compute dot products\n",
    "        sapwood_logits = tf.reduce_sum(fc_weights[0] * sapwood_proj)\n",
    "        heartwood_logits = tf.reduce_sum(fc_weights[1] * heartwood_proj)\n",
    "\n",
    "        # Add embedding logits to CNN logits\n",
    "        logits = logits + tf.stack([sapwood_logits, heartwood_logits])\n",
    "\n",
    "        return logits\n",
    "'''\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, base_model, sapwood_emb, heartwood_emb):\n",
    "        \n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        # Pre-computed embeddings for labels\n",
    "        self.sapwood_emb = tf.constant(sapwood_emb.detach().cpu().numpy(), dtype=tf.float32)\n",
    "        self.heartwood_emb = tf.constant(heartwood_emb.detach().cpu().numpy(), dtype=tf.float32)\n",
    "\n",
    "        # Projection layer to align dimensions\n",
    "        self.projection_layer = tf.keras.layers.Dense(128, activation=None)  # Example size\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Base model output\n",
    "        logits = self.base_model(inputs)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Ensure embeddings are 2D before projection\n",
    "        sapwood_emb_reshaped = tf.expand_dims(self.sapwood_emb, axis=0)  # Add batch dimension\n",
    "        sapwood_proj = self.projection_layer(sapwood_emb_reshaped)\n",
    "\n",
    "        heartwood_emb_reshaped = tf.expand_dims(self.heartwood_emb, axis=0)\n",
    "        heartwood_proj = self.projection_layer(heartwood_emb_reshaped)\n",
    "\n",
    "        #print(\"Sapwood projection shape:\", sapwood_proj.shape)\n",
    "        #print(\"Heartwood projection shape:\", heartwood_proj.shape)\n",
    "        \n",
    "        '''\n",
    "        # Extract FC layer weights\n",
    "        fc_weights = self.base_model.layers[-1].kernel  # Shape: (2, 128)\n",
    "\n",
    "        # Compute dot products\n",
    "        sapwood_logits = tf.reduce_sum(fc_weights[0] * sapwood_proj)\n",
    "        heartwood_logits = tf.reduce_sum(fc_weights[1] * heartwood_proj)\n",
    "        \n",
    "        fc_weights[0].shape\n",
    "\n",
    "        # Add embedding logits to CNN logits\n",
    "        logits = logits + tf.stack([sapwood_logits, heartwood_logits])\n",
    "        '''\n",
    "\n",
    "        # Optionally, combine outputs or perform other operations\n",
    "        return logits, sapwood_proj, heartwood_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model (CNN)\n",
    "input_shape = (32, 32, optimal_components, 1)\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    \n",
    "    layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.01), input_shape=input_shape),\n",
    "    \n",
    "    layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "    \n",
    "    layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    \n",
    "    layers.Conv3D(128, kernel_size=(3, 3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    \n",
    "    layers.AveragePooling3D(pool_size=(2, 2, 2)),\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(128, activation='softmax', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    #layers.Dropout(0.5),\n",
    "    \n",
    "    #layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    \n",
    "    #layers.Dense(2, activation=None)  # Remove activation for logits\n",
    "])\n",
    "\n",
    "# Custom model\n",
    "model = CustomModel(base_model = cnn_model, sapwood_emb = sapwood_emb_projected, heartwood_emb = heartwood_emb_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output shape: (8, 128)\n",
      "Sapwood projection shape: (1, 128)\n",
      "Heartwood projection shape: (1, 128)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((8, 32, 32, 199, 1))  # Batch of 8\n",
    "output = model(x)\n",
    "\n",
    "print(\"Base model output shape:\", output[0].shape)  # Output from base model\n",
    "print(\"Sapwood projection shape:\", output[1].shape)  # Should be (1, 128)\n",
    "print(\"Heartwood projection shape:\", output[2].shape)  # Should be (1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy_1/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 542, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 531, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 775, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\rafin\\AppData\\Local\\Temp\\ipykernel_10360\\1577180523.py\", line 19, in <module>\n      history = model.fit(\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy_1/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [1,128] and labels shape [8]\n\t [[{{node sparse_categorical_crossentropy_1/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_10461]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 19\u001b[0m\n\u001b[0;32m      4\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      5\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_test.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,    \n\u001b[0;32m      6\u001b[0m     monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,          \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m early_stopping_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     14\u001b[0m     monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m     16\u001b[0m     restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy_1/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 542, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 531, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 775, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\rafin\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\rafin\\AppData\\Local\\Temp\\ipykernel_10360\\1577180523.py\", line 19, in <module>\n      history = model.fit(\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy_1/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [1,128] and labels shape [8]\n\t [[{{node sparse_categorical_crossentropy_1/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_10461]"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss= SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Define the callbacks\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = 'best_model_test.h5',    \n",
    "    monitor = 'accuracy',          \n",
    "    save_best_only = True,              \n",
    "    save_weights_only = False,             \n",
    "    mode = 'max',                          \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    patience = 15,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    \n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true labels and predictions\n",
    "test_labels = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "predictions = np.argmax(model.predict(test_dataset), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[str(i) for i in range(2)], yticklabels=[str(i) for i in range(2)])\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
